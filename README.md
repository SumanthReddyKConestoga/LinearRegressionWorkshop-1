````markdown
# ğŸ­ Linear Regression Architecture Workshop â€” Robot Failure Prediction (MLOps Ready)

## Executive Summary
This project builds a **Univariate Linear Regression** solution to **predict when the next robot failure is likely to occur** using robot sensor signals (Axis measurements).  
We implement Linear Regression in two waysâ€”**from scratch** (gradient descent) and **scikit-learn**â€”then compare performance and generate clear plots as evidence.  
The project follows **MLOps-style architecture**: modular code, config-driven experiments, reproducible runs, and experiment tracking.

---

## ğŸ¯ Problem Statement
Manufacturing robots generate continuous sensor data. Failures are costly and often detected too late.  
Our goal is to use one key sensor feature (example: `Axis #1`) to predict:

âœ… **Time remaining until the next failure event** (e.g., `time_to_failure_days`)

This supports **predictive maintenance** by enabling proactive alerts (e.g., â€œraise an alert ~2 weeks before failureâ€).

---

## âœ… What We Built (Workshop Deliverables)
### Session 1 â€” Linear Regression
- Loaded robot CSV into Pandas and inspected data quality
- Preprocessed data (missing values, normalization, train/test split)
- Implemented **manual Linear Regression** (MSE + Gradient Descent)
- Implemented **scikit-learn Linear Regression** for comparison
- Evaluated with: **RMSE, MAE, RÂ²**
- Produced regression plots to show model fit

### Session 2 â€” MLOps Architecture
- Refactored notebook logic into modular scripts (`src/`)
- Parameterized experiments using YAML config (`configs/experiment_config.yaml`)
- Saved experiment outputs to:
  - `experiments/results.csv` (metrics tracking)
  - `experiments/plots/` (visual proof)
- Ensured reproducibility: anyone can clone + run and get the same outputs

---

## ğŸ“Œ How â€œFailureâ€ is Defined in This Workshop
The dataset does not include an explicit `failure = 1` column.  
So we define failure events **from abnormal sensor behavior**, using a simple and explainable rule:

- Compute anomaly score (e.g., rolling z-score) on a selected axis
- Mark a **failure event** when the sensor deviation crosses a threshold
- Compute target label:

âœ… `time_to_failure_days = (next_failure_time - current_time)`

Then Linear Regression learns:

**Sensor Axis value â†’ time remaining until next failure**

---

## ğŸ—‚ï¸ Project Folder Structure
```text
LinearRegressionArchitecture_Workshop1/
â”‚â”€â”€ configs/
â”‚   â””â”€â”€ experiment_config.yaml
â”‚
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ RMBR4-2_export_test.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ processed_robot_data.csv
â”‚
â”‚â”€â”€ experiments/
â”‚   â”œâ”€â”€ results.csv
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ robot_pm_univariate_v1_scratch_scatter_line.png
â”‚       â”œâ”€â”€ robot_pm_univariate_v1_scratch_residuals.png
â”‚       â”œâ”€â”€ robot_pm_univariate_v1_sklearn_scatter_line.png
â”‚       â””â”€â”€ robot_pm_univariate_v1_sklearn_residuals.png
â”‚
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ linear_regression.ipynb
â”‚   â””â”€â”€ RobotPM_MLOps.ipynb
â”‚
â”‚â”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”‚
â”‚â”€â”€ scripts/
â”‚   â””â”€â”€ generate_notebooks.py
â”‚
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â””â”€â”€ run_experiment.py
â”‚
â”‚â”€â”€ requirements.txt
â””â”€â”€ README.md
````

---

## ğŸ§  What Each Module Does (3 lines each)

### `src/data_loader.py`

* Loads robot sensor data from CSV (and supports DB/API expansion).
* Ensures consistent column formats and clean DataFrame output.
* Supplies the raw input needed for failure-time prediction.

### `src/preprocessing.py`

* Cleans missing values, sorts by time, normalizes features.
* Creates the prediction label: **time until next failure**.
* Outputs model-ready X (sensor axis) and y (time-to-failure).

### `src/model.py`

* Implements Linear Regression **from scratch** using gradient descent.
* Runs scikit-learn LinearRegression as the baseline comparison.
* Produces predicted values of **time until next failure**.

### `src/evaluation.py`

* Computes RMSE, MAE, and RÂ² to measure model quality.
* Generates regression plots and residual diagnostics.
* Saves metrics and graphs as proof of failure prediction performance.

### `src/run_experiment.py`

* Orchestrates the full pipeline using YAML configuration.
* Runs preprocessing â†’ training â†’ evaluation â†’ saves outputs.
* Produces repeatable results to predict **next failure timing**.

### `configs/experiment_config.yaml`

* Stores all experiment parameters (data path, feature axis, thresholds, learning rate).
* Enables reruns without changing code (config-driven workflow).
* Defines what â€œfailure predictionâ€ means for a given experiment.

---

## ğŸ“Š Outputs Produced

### 1) Experiment Tracking

* `experiments/results.csv`

  * Contains metrics for scratch vs sklearn models:

    * RMSE, MAE, RÂ²
    * run_tag timestamp

### 2) Visual Proof (Plots)

Saved under `experiments/plots/`:

* **Scatter + Regression line** (model fit)
* **Residual plot** (error distribution)
* (Optional) time-series with failure markers if enabled

---

## â–¶ï¸ How to Run the Project (Step-by-step)

### 1) Create and activate virtual environment

```powershell
python -m venv .venv
.\.venv\Scripts\activate
```

### 2) Install dependencies

```powershell
pip install -r requirements.txt
```

### 3) Run the full pipeline (recommended)

```powershell
python -m src.run_experiment
```

### 4) Open outputs

* Metrics: `experiments/results.csv`
* Plots: `experiments/plots/`

---

## ğŸ““ Notebooks (For Presentation)

* `notebooks/EDA.ipynb`
  Data understanding: missing values, feature selection, trends

* `notebooks/linear_regression.ipynb`
  Manual LR vs sklearn comparison, plots, and metrics

* `notebooks/RobotPM_MLOps.ipynb`
  Documents the MLOps refactor, config-driven execution, tracking outputs

---

## ğŸ–¥ï¸ (Optional) Run Dashboard

If you want a UI to view the dataset/stream:

```powershell
streamlit run dashboard/app.py
```

---

## ğŸ§¾ Key Design Decisions (MLOps)

* **Separation of concerns:** loader, preprocessing, model, evaluation are independent modules
* **Config-driven:** all tunable values live in YAML (no hard-coded magic values)
* **Experiment tracking:** results saved in `experiments/results.csv`
* **Reproducibility:** same config + same code = same output plots and metrics

---
